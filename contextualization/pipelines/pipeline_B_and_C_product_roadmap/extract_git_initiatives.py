import json
import logging
from pathlib import Path
from typing import Any

import pandas as pd
from langchain_core.runnables import Runnable
from otel_extensions import instrumented

from contextualization.conf.config import conf, llm_name
from contextualization.pipelines.pipeline_B_and_C_product_roadmap.combining_results import (
    summarize_git_initiatives,
)
from contextualization.pipelines.pipeline_B_and_C_product_roadmap.prompts.filter import (
    filter_columns,
)
from contextualization.pipelines.pipeline_B_and_C_product_roadmap.prompts.git_L1L2initiatives_prompt import (
    git_initiatives_chain,
)
from contextualization.tools.llm_tools import (
    calculate_token_count_async,
    get_batches_to_merge,
)

batch_threshold = conf["llms"][llm_name]["batch_threshold"]
token_limit = conf["llms"][llm_name]["token_limit"]


async def analyze_commits_with_task(
    df_chunks: list[pd.DataFrame],
    selected_columns: dict[str, Any],
    commit_analyser_chain: Runnable,
    task: str,
    chat_input: str | None = None,
) -> list[dict[str, Any]]:
    chat_prompt = ""
    if chat_input:
        chat_prompt = "5. Pay particular attention to this custom prompt provided by the client: " + chat_input
    # Filter columns based on the task
    # selected_columns = filter_columns(dfs[0], task)

    # Filter columns based on the task(Hardcoded column)
    # selected_columns = {'columns':['name','Summary', 'Categorization_of_Changes', 'Impact_on_product','Purpose_of_change']}
    # Construct the schema with column names and rows
    column_names = " | ".join(df_chunks[0][selected_columns["columns"]].columns.to_list())
    chunks = []
    for df in df_chunks:
        row_str = "\n".join(
            df[selected_columns["columns"]].apply(lambda x: " | ".join(map(str, x)).replace("\n", " "), axis=1).values
        )

        chunks.append(row_str)

    outputs = []
    # Determine how many chunks to process per batch
    chunks_per_batch = max(1, batch_threshold // token_limit)
    # Assuming 'chunks' is a list of chunks to process
    for i in range(0, len(chunks), chunks_per_batch):
        # Select the current batch of chunks
        chunk_batch = chunks[i : i + chunks_per_batch]
        logging.info(f"Processing {len(chunk_batch)} chunk(s) out of total {len(chunks)}..")
        try:
            output = await commit_analyser_chain.abatch(
                [
                    {
                        "csv_schema": column_names,
                        "csv": chunk,
                        "task": task,
                        "chat_prompt": chat_prompt,
                    }
                    for chunk in chunk_batch
                ]
            )

            outputs.append(output)
        except Exception as e:
            logging.exception(f"Pipeline B/C - Error processing chunks", extra={"chunk_number": i})
            continue

    return outputs


@instrumented
async def analyze_git_commits(
    file_path: Path, df: pd.DataFrame, chat_input: str | None = None
) -> tuple[dict[str, Any], Path]:
    """
    Analyze and summarize Git commit history data.

    Args:
        file_path (str): Path to the CSV file containing the Git data.
        df: Git data with summary.
        chat_input (str, optional): Custom input for the analysis.

    Returns:
        str: The response generated by the commit analyzer chain.
    """
    task = """Given the extracted GitHub commit history, identify and summarize the main initiatives (L1) and epics (L2) from the data. 
    Extract clear patterns of related work to determine the strategic development roadmap."""
    file_path = Path(file_path)

    # Filter columns based on the task
    selected_columns = await filter_columns(df, task)

    # Calculate the tik_tokens
    df = await calculate_token_count_async(
        df, text_columns=selected_columns["columns"], token_column="output_tik_tokens"
    )
    logging.info(f"Git Dataframe shape with token count column added: {df.shape}")

    # Split the DataFrame into chunks (assuming get_batches_to_merge function is defined)
    df_chunks = get_batches_to_merge(df, tiktoken_column="output_tik_tokens")
    logging.info(f"Number of chunks of git data: {len(df_chunks)}")

    # Analyze commits using the provided analyzer and filter columns
    outputs = await analyze_commits_with_task(df_chunks, selected_columns, git_initiatives_chain, task, chat_input)

    # Flatten the output list (assuming output is a list of lists)
    flat_output = [item for sublist in outputs for item in sublist]

    output_file = file_path.parent / f"{file_path.stem}_git_data_initiatives.json"

    # Write the response to a JSON file
    with open(output_file, "w") as f:
        json.dump(flat_output, f, indent=4)

    logging.info(f"Saving the list of git initiatives json to a file: {output_file} ")
    output_file_path = file_path.parent / f"{file_path.stem}_git_data_initiatives_combined.json"
    summary = await summarize_git_initiatives(flat_output, output_file_path)

    return summary, output_file_path
